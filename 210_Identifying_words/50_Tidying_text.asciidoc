[[char-filters]]
=== 整理输入文本


当输入文本是干净的时候分词器提供最佳分词结果，有效文本，这里 _有效_ 指的是遵从Unicode算法期望的标点符号规则((("text", "tidying up text input for tokenizers")))((("words", "identifying", "tidying up text input")))。
很多时候，然而我们需要处理的文本会是除了干净文本之外的任何东西。在分词之前整理文本会提升输出结果的质量。

==== 分词 HTML

将HTML通过 `标准分词器` 或 `icu_分词器` 分词将产生糟糕的结果((("HTML, tokenizing")))。这些分词器不知道如果处理HTML标签。例如：

[source,js]
--------------------------------------------------
GET /_analyzer?tokenizer=standard
<p>Some d&eacute;j&agrave; vu <a href="http://somedomain.com>">website</a>
--------------------------------------------------

`标准分词器` ((("standard tokenizer", "tokenizing HTML")))会混淆HTML标签和实体，并且输出一下词汇单元： `p` ， `Some` ， `d` ， `eacute` ， `j` ， `agrave` ， `vu` ， `a` ，
`href` ， `http` ， `somedomain.com` ， `website` ， `a` 。这些词汇单元显然不知所云！


_字符过滤器_ 可以添加进分析器中，在将文本传给分词器之前预处理该文本。在这种情况下，我们可以用 `html_strip` 字符过滤器((("analyzers", "adding character filters to")))((("html_strip character filter")))移除HTML标签并编码HTML实体如 `&eacute;` 为一致的Unicode字符。


字符过滤器可以通过 `analyze` API进行测试，这需要在查询字符串中指明他们：

[source,js]
--------------------------------------------------
GET /_analyzer?tokenizer=standard&char_filters=html_strip
<p>Some d&eacute;j&agrave; vu <a href="http://somedomain.com>">website</a>
--------------------------------------------------

想将它们作为分析器的一部分使用，需要把它们添加到分词器定义里：

[source,js]
--------------------------------------------------
PUT /my_index
{
    "settings": {
        "analysis": {
            "analyzer": {
                "my_html_analyzer": {
                    "tokenizer":     "standard",
                    "char_filter": [ "html_strip" ]
                }
            }
        }
    }
}
--------------------------------------------------


一旦创建好之后， 我们新的 `my_html_analyzer` 就可以用 `analyze` API 测试：

[source,js]
--------------------------------------------------
GET /my_index/_analyzer?analyzer=my_html_analyzer
<p>Some d&eacute;j&agrave; vu <a href="http://somedomain.com>">website</a>
--------------------------------------------------


这次输出的词汇单元才是我们期望的： `Some` ， ++déjà++ ， `vu` ， `website` 。

==== 整理标点符号

The `standard` tokenizer and `icu_tokenizer` both understand that an
apostrophe _within_ a word should be treated as part of the word, while single
quotes that _surround_ a word should not.((("standard tokenizer", "handling of punctuation")))((("icu_tokenizer", "handling of punctuation")))((("punctuation", "tokenizers&#x27; handling of"))) Tokenizing the text `You're my 'favorite'`. would correctly emit the tokens `You're, my, favorite`.

Unfortunately,((("apostrophes"))) Unicode lists a few characters that are sometimes used
as apostrophes:

`U+0027`::
      Apostrophe (`'`)&#x2014;the original ASCII character

`U+2018`::
      Left single-quotation mark (`‘`)&#x2014;opening quote when single-quoting

`U+2019`::
      Right single-quotation mark (`’`)&#x2014;closing quote when single-quoting, but also the  preferred character to use as an apostrophe

Both tokenizers treat these three characters as an apostrophe (and thus as
part of the word) when they appear within a word. Then there are another three
apostrophe-like characters:

`U+201B`::
      Single high-reversed-9 quotation mark (`‛`)&#x2014;same as `U+2018` but differs in appearance

`U+0091`::
      Left single-quotation mark in ISO-8859-1&#x2014;should not be used in Unicode

`U+0092`::
      Right single-quotation mark in ISO-8859-1&#x2014;should not be used in Unicode

Both tokenizers treat these three characters as word boundaries--a place to
break text into tokens.((("quotation marks"))) Unfortunately, some publishers use `U+201B` as a
stylized way to write names like `M‛coy`, and the second two characters may well
be produced by your word processor, depending on its age.

Even when using the ``acceptable'' quotation marks, a word written with a
single right quotation mark&#x2014;`You’re`&#x2014;is not the same as the word written
with an apostrophe&#x2014;`You're`&#x2014;which means that a query for one variant
will not find the other.

Fortunately, it is possible to sort out this mess with the `mapping` character
filter,((("character filters", "mapping character filter")))((("mapping character filter"))) which allows us to replace all instances of one character with
another.  In this case, we will replace all apostrophe variants with the
simple `U+0027` apostrophe:

[source,js]
--------------------------------------------------
PUT /my_index
{
  "settings": {
    "analysis": {
      "char_filter": { <1>
        "quotes": {
          "type": "mapping",
          "mappings": [ <2>
            "\\u0091=>\\u0027",
            "\\u0092=>\\u0027",
            "\\u2018=>\\u0027",
            "\\u2019=>\\u0027",
            "\\u201B=>\\u0027"
          ]
        }
      },
      "analyzer": {
        "quotes_analyzer": {
          "tokenizer":     "standard",
          "char_filter": [ "quotes" ] <3>
        }
      }
    }
  }
}
--------------------------------------------------
<1> We define a custom `char_filter` called `quotes` that
    maps all apostrophe variants to a simple apostrophe.
<2> For clarity, we have used the JSON Unicode escape syntax
    for each character, but we could just have used the
    characters themselves: `"‘=>'"`.
<3> We use our custom `quotes` character filter to create
    a new analyzer called `quotes_analyzer`.

As always, we test the analyzer after creating it:

[source,js]
--------------------------------------------------
GET /my_index/_analyze?analyzer=quotes_analyzer
You’re my ‘favorite’ M‛Coy
--------------------------------------------------

This example returns the following tokens, with all of the in-word
quotation marks replaced by apostrophes: `You're`, `my`, `favorite`, `M'Coy`.

The more effort that you put into ensuring that the tokenizer receives good-quality input, the better your search results will be.
