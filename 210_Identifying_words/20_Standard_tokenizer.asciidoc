[[standard-tokenizer]]
=== 标准分词器

分词器接受一个字符串作为输入，将((("words", "identifying", "using standard tokenizer")))((("standard tokenizer")))((("tokenizers")))这个字符串拆分成单独的词或 _词汇单元_
（可以舍弃一些标点符号），并且输出一个词汇单元流。


有趣的是词汇识别的算法。 `whitespace` （空格）分词器((("whitespace tokenizer")))按空格 -- 空白、tabs、换行符等等拆分 -- 想象下一个连续的非空格字符组成一个词汇单元。例如：

[source,js]
--------------------------------------------------
GET /_analyze?tokenizer=whitespace
You're the 1st runner home!
--------------------------------------------------

这个请求会返回如下词:

`You're` 、 `the` 、 `1st` 、 `runner` 、 `home!`


`字符分词器` ，在另一方面，按照任何非字符进行拆分，这样的话((("letter tokenizer")))将会返回如下单词： `You` 、 `re` 、 `the` 、 `st` 、 `runner` 、 `home` 。



`标准分词器` ((("Unicode Text Segmentation algorithm"))) 使用 Unicode 文本分割算法（定义来源于 http://unicode.org/reports/tr29/[Unicode Standard Annex #29]）来寻找单词间的界限，并且输出界限之间的内容。 Unicode 技术使其可以成功的对包含混合语言的文本进行分词。


标点符号可能是单词的一部分，也可能不是，这取决于它出现的位置：

[source,js]
--------------------------------------------------
GET /_analyze?tokenizer=standard
You're my 'favorite'.
--------------------------------------------------


在这个例子中，`You're` 中的撇号被视为单词的一部分，然而 `'favorite'` 中的单引号则不会被视为单词的一部分，所以分词结果如下： `You're` 、 `my` 、 `favorite` 。

[TIP]
==================================================


`uax_url_email` （标准Email URL分词器）分词的方式和 `标准分词器` 一样。不过它只能识别((("email addresses and URLs, tokenizer for"))) email 地址和 URLs 并且将这些输出为单个词汇单元。 `标准分词器` 则不一样，会将 email 地址和 URLs 拆分成独立的单词，例如，email 地址 `joe-bloggs@foo-bar.com` 的分词结果为 `joe` 、 `bloggs` 、 `foo` 、 `bar.com` 。

==================================================



`标准分词器` 是大多数语言分词的一个合理的起点，特别是西方语言。事实上，它是很多具体语言的分词器的基础，如 `英语` 、`法语` 、和 `西班牙语` 分析器。它也支持亚洲语言，只是有些缺陷，你可以考虑替换为 `icu_分词器` ，在 ICU 插件中可用。
