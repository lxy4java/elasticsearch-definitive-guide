[[standard-tokenizer]]
=== 标准分词器

分词器接受一个字符串作为输入，将((("words", "identifying", "using standard tokenizer")))((("standard tokenizer")))((("tokenizers")))这个字符串拆分成单独的词，或 _词汇单元_
（也许可以描述成类似标点的符号），并且输出一个词汇单元流作为输出。




有趣的是词汇识别的算法。 `whitespace` （空格）分词器((("whitespace tokenizer")))仅仅按空格 -- 空白，tabs，换行符等等拆分 -- 想象下单个词汇单元的连续的无空格字符串。例如：


[source,js]
--------------------------------------------------
GET /_analyze?tokenizer=whitespace
You're the 1st runner home!
--------------------------------------------------

这个请求会返回如下词:

`You're` ， `the` ， `1st` ， `runner` ， `home!`


`字符分词器` ，在另一方面，按照任何非字符进行拆分，这样的话将会返回((("letter tokenizer")))如下单词： `You` ， `re` ， `the` ， `st` ， `runner` ， `home` 。



`标准分词器` ((("Unicode Text Segmentation algorithm"))) 使用Unicode文本分割算法（定义在 http://unicode.org/reports/tr29/[Unicode Standard Annex #29]）来寻找单词间的界限，并且输出界限之间的任何东西。Unicode技术使其可以成功的分词包含混合语言的文本。


标点符号可能是单词的一部分，也可能不是，这取决于它出现的位置：

[source,js]
--------------------------------------------------
GET /_analyze?tokenizer=standard
You're my 'favorite'.
--------------------------------------------------


在这个例子中，`You're` 中的撇号被视为单词的一部分，然而 `'favorite'` 中的单引号则不会被视为单词的一部分，所以分词结果如下： `You're` ， `my` ， `favorite` 。

[TIP]
==================================================


`uax_url_email` （标准Email URL分词器）分词的方式和 `标准分词器` 一样。不过它只能识别email地址和URLs并且将这些输出为单个词汇单元。 `标准分词器` 则不一样，会将email地址和URLs拆分成独立的单词，例如，email地址 `joe-bloggs@foo-bar.com` 的分词结果为 `joe` ， `bloggs` ， `foo` ， `bar.com` 。

==================================================



`标准分词器` 是大多数语言分词的一个合理的起点，特别是西方语言。事实上，它是很多具体语言的分词器的基础，如 `英语` 、`法语` 、和 `西班牙语` 分析器。它也支持亚洲语言，只是有些缺陷，你可以考虑替换为 `icu_分词器` ，在ICU插件中可用。
